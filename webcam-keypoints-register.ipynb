{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce0c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.insert(19, 'litepose-pose-estimation/src')\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import lp_config.lp_common_config as cc\n",
    "from lp_model.lp_litepose import LitePose\n",
    "from lp_inference.lp_inference import inference, assocEmbedding\n",
    "from lp_utils.lp_image_processing import drawHeatmap, drawKeypoints, drawSkeleton\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imutils\n",
    "sys.path.insert(20, 'src/referee_gloves_detector')\n",
    "from color_labeler import ColorLabeler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggo il file di configurazione\n",
    "with open('config/config.json') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Ottengo i percorsi dei file\n",
    "file_path_big_arch = config_data['path_big_arch']\n",
    "file_path_csv_keypoints_webcam = config_data['path_csv_keypoints_webcam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitePose().to(cc.config[\"device\"])\n",
    "model.load_state_dict(torch.load(file_path_big_arch, map_location=cc.config[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65804b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Filtering\n",
    "\n",
    "image = cv2.imread('C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\bloisi_nao_red_hands.png')\n",
    "cv2.imshow(\"Original\", image)\n",
    " \n",
    "result = image.copy()\n",
    " \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    " \n",
    "lower1 = np.array([0, 100, 100])\n",
    "upper1 = np.array([5, 255, 255])\n",
    " \n",
    "lower2 = np.array([174,100,100])\n",
    "upper2 = np.array([179,255,255])\n",
    " \n",
    "lower_mask = cv2.inRange(image, lower1, upper1)\n",
    "upper_mask = cv2.inRange(image, lower2, upper2)\n",
    " \n",
    "full_mask = lower_mask + upper_mask;\n",
    " \n",
    "result = cv2.bitwise_and(result, result, mask=full_mask)\n",
    " \n",
    "cv2.imshow('mask', full_mask)\n",
    "cv2.imshow('result', result)\n",
    " \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc08ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation\n",
    "# TODO: migliorare o togliere del tutto, infondo a cosa dovrebbe servirci?\n",
    "\n",
    "# Converto l'immagine in grayscale\n",
    "gray_image = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "# Converto l'immagine da grayscale a binaria\n",
    "ret,thresh = cv2.threshold(gray_image,127,255,0)\n",
    " \n",
    "# Trovo i contorni nell'immagine binaria\n",
    "contours, im2 = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "for c in contours:\n",
    "    # Calcolo il centroide\n",
    "    M = cv2.moments(c)\n",
    " \n",
    "    # Calcolo le coordinate x e y del centroide\n",
    "    if M[\"m00\"] != 0:\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "    else:\n",
    "        cX, cY = 0, 0\n",
    "    cv2.circle(result, (cX, cY), 5, (255, 255, 255), -1)\n",
    "    cv2.putText(result, \"centroid\", (cX - 25, cY - 25),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"Image\", result)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba612c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping\n",
    "\n",
    "def leftmost_red_pixel(numpy_image):\n",
    "\n",
    "    image = Image.fromarray(numpy_image.astype('uint8'), 'RGB')\n",
    "    width, height = image.size\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            r, g, b = image.getpixel((x, y))\n",
    "\n",
    "            if r > 0:\n",
    "                return (x, y)\n",
    "\n",
    "    return (None,None)\n",
    "\n",
    "def rightmost_red_pixel(numpy_image):\n",
    "\n",
    "    image = Image.fromarray(numpy_image.astype('uint8'), 'RGB')\n",
    "    width, height = image.size\n",
    "\n",
    "    for x in range(width - 1, -1, -1):\n",
    "        for y in range(height):\n",
    "            r, g, b = image.getpixel((x, y))\n",
    "\n",
    "            if r > 0:\n",
    "                return (x, y)\n",
    "\n",
    "    return (None,None)\n",
    "\n",
    "print(result.shape)\n",
    "im_pil_1 = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "(xs,ys) = leftmost_red_pixel(result)\n",
    "(xd,yd) = rightmost_red_pixel(result)\n",
    "if(xs != None and ys != None and (ys-10)>0):\n",
    "    col = ys-10\n",
    "    red_filtered_image_restricted = result[:,col:,:]\n",
    "    print(red_filtered_image_restricted.shape)\n",
    "if(xd != None and yd != None and (yd+10)<(np.shape(red_filtered_image_restricted)[1])):\n",
    "    col = yd+10\n",
    "    red_filtered_image_restricted = red_filtered_image_restricted[:,:col,:]\n",
    "    print(red_filtered_image_restricted.shape)\n",
    "im_pil_2 = cv2.cvtColor(red_filtered_image_restricted, cv2.COLOR_BGR2RGB)\n",
    "f, axarr = plt.subplots(nrows=1, ncols=2)\n",
    "axarr[0].imshow(im_pil_1)\n",
    "axarr[1].imshow(im_pil_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\example_shapes.png\"\n",
    "# uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\example_shapes_1.png\"\n",
    "# uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\example_shapes_2.png\"\n",
    "uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\bloisi_nao_red_hands.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apro la webcam e salvo al volo i frame e il timestamp\n",
    "\n",
    "webcam=cv2.VideoCapture(0) \n",
    "\n",
    "if not webcam.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura della webcam\")\n",
    "\n",
    "keypoints_vec = []\n",
    "timestamps = []\n",
    "resize = transforms.Resize([224, 224])  \n",
    "to_tensor = transforms.ToTensor()\n",
    "ret,frame=webcam.read()\n",
    "while ret:\n",
    "    \n",
    "    \n",
    "    istante_attuale = datetime.now()\n",
    "    stringa_istante = istante_attuale.strftime(\"%d/%m/%Y %H:%M:%S.%f\")\n",
    "    timestamps.append(stringa_istante) \n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = img[:,80:]\n",
    "    img = img[:,:-80]\n",
    "    \n",
    "\n",
    "    im_pil = Image.fromarray(img)\n",
    "    frame = resize(im_pil)\n",
    "    tensor = to_tensor(frame)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    if ret==True:\n",
    "        output, keypoints = inference(model, tensor)\n",
    "        keypoints_vec.append(keypoints)\n",
    "        embedding = assocEmbedding(keypoints)\n",
    "        frame_modified = drawSkeleton(tensor[0], embedding[0])\n",
    "        #frame = drawKeypoints(tensor[0], keypoints[0])\n",
    "        cv2.imshow(\"Pose estimation\", frame_modified)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break         \n",
    "    ret, frame = webcam.read()\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Dimensioni dell'array di frame:\", len(keypoints_vec))\n",
    "print(\"Dimensioni dell'array di timestamp:\", len(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo solo i 9 keypoints scelti per ogni frame\n",
    "\n",
    "d = {'timestamp':[],'x': [], 'y': [], 'tag':[]}\n",
    "df_keypoints = pd.DataFrame(data=d)\n",
    "for timestamp, restricted_keypoints in zip(timestamps, keypoints_vec):\n",
    "    for i in range(len(restricted_keypoints[0])):\n",
    "        if not (restricted_keypoints[0][i]):\n",
    "            df_temp = pd.DataFrame([{'timestamp': '01/01/1970 01:00:00','x':np.nan, 'y':np.nan, 'tag':np.nan}])\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "        else:\n",
    "            df_temp = pd.DataFrame(restricted_keypoints[0][i])\n",
    "            df_temp['timestamp'] = timestamp\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "df_keypoints.to_csv(file_path_csv_keypoints_webcam, index=False)\n",
    "df_multiindex = df_keypoints.set_index(['timestamp'])\n",
    "display(df_multiindex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
