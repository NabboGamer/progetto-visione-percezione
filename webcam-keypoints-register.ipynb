{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ce0c0a",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-20T21:17:26.491187700Z",
     "start_time": "2023-06-20T21:17:21.055301600Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(19, 'litepose-pose-estimation/src')\n",
    "sys.path.insert(20, 'src/referee_gloves_detector')\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import lp_config.lp_common_config as cc\n",
    "from lp_model.lp_litepose import LitePose\n",
    "from lp_inference.lp_inference import inference, assocEmbedding\n",
    "from lp_utils.lp_image_processing import drawHeatmap, drawKeypoints, drawSkeleton\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import red_filtering, segmentation_and_cropping, equalizing, squaring\n",
    "import math\n",
    "from Homography import Homography\n",
    "import copy\n",
    "from imutils.video import VideoStream\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggo il file di configurazione\n",
    "with open('config/config.json') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Ottengo i percorsi dei file\n",
    "file_path_big_arch = config_data['path_big_arch']\n",
    "file_path_csv_keypoints = config_data['path_csv_keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitePose().to(cc.config[\"device\"])\n",
    "model.load_state_dict(torch.load(file_path_big_arch, map_location=cc.config[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HOMOGRAPHY IMAGE AUTOMATICALLY\n",
    "\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\resources\\\\bloisi_nao_red_hands.png\"\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\test\\\\test_1.jpg\"\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\test\\\\test_2.jpg\"\n",
    "uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\test\\\\test_3.jpg\"\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\test\\\\test_4.jpg\"\n",
    "\n",
    "img = Image.open(uri).convert('RGB')\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "to_tensor = transforms.ToTensor()\n",
    "tensor = to_tensor(img)\n",
    "tensor = tensor.unsqueeze(0)\n",
    "output, keypoints = inference(model, tensor)\n",
    "restricted_keypoints = [[keypoints[0][0], keypoints[0][1], keypoints[0][6], keypoints[0][7], keypoints[0][12],keypoints[0][13]]]\n",
    "idx = 0\n",
    "img = drawKeypoints(tensor[idx], restricted_keypoints[idx])\n",
    "cv2.imshow(\"Pose estimation\", img)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img, 'RGB')\n",
    "im_pil.save('src/referee_gloves_detector/resources/src.jpg')\n",
    "src = cv2.imread(\"src/referee_gloves_detector/resources/src.jpg\")\n",
    "dst = cv2.imread(\"src/referee_gloves_detector/resources/dst.jpg\")\n",
    "h = Homography(src,dst)\n",
    "#punti2d_old = [[263, 121], [74, 121], [212, 330], [116, 330], [229, 431], [101, 431], [228, 529], [100, 529], [166, 41], [166, 114]]\n",
    "punti2d = [[412, 271], [226, 271], [363, 481], [264, 481], [317, 191], [317, 266]]\n",
    "punti3d = []\n",
    "index_list = [0, 1, 6, 7, 12, 13]\n",
    "count = 0\n",
    "index_to_remove = []\n",
    "for i in index_list:\n",
    "    try:\n",
    "        punti3d.append([keypoints[0][i][0]['x'], keypoints[0][i][0]['y']])\n",
    "    except:\n",
    "        index_to_remove.append(count)\n",
    "    finally:\n",
    "        count = count + 1\n",
    "for index in sorted(index_to_remove, reverse=True):\n",
    "    del punti2d[index]\n",
    "corr = h.normalize_points(punti2d,punti3d)\n",
    "h._compute_view_based_homography(corr)\n",
    "plan_view = cv2.warpPerspective(src, h.H, (dst.shape[1], dst.shape[0]))\n",
    "cv2.imshow(\"Pose estimation homography\", plan_view)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# warm up the camera sensor\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(2.0)\n",
    "# pip install opencv-contrib-python\n",
    "fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "\n",
    "# number of frames is a variable for development purposes, you can change the for loop to a while(cap.isOpened()) instead to go through the whole video\n",
    "num_frames = 350\n",
    "\n",
    "first_iteration_indicator = 1\n",
    "for i in range(0, num_frames):\n",
    "    '''\n",
    "    There are some important reasons this if statement exists:\n",
    "        -in the first run there is no previous frame, so this accounts for that\n",
    "        -the first frame is saved to be used for the overlay after the accumulation has occurred\n",
    "        -the height and width of the video are used to create an empty image for accumulation (accum_image)\n",
    "    '''\n",
    "    if (first_iteration_indicator == 1):\n",
    "        ret,frame=cap.read()\n",
    "        first_frame = copy.deepcopy(frame)\n",
    "        #print(type(frame))\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        height, width = gray.shape[:2]\n",
    "        accum_image = np.zeros((height, width), np.uint8)\n",
    "        first_iteration_indicator = 0\n",
    "    else:\n",
    "        ret,frame=cap.read()  # read a frame\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # convert to grayscale\n",
    "\n",
    "        fgmask = fgbg.apply(gray)  # remove the background\n",
    "\n",
    "        # for testing purposes, show the result of the background subtraction\n",
    "        # cv2.imshow('diff-bkgnd-frame', fgmask)\n",
    "\n",
    "        # apply a binary threshold only keeping pixels above thresh and setting the result to maxValue.  If you want\n",
    "        # motion to be picked up more, increase the value of maxValue.  To pick up the least amount of motion over time, set maxValue = 1\n",
    "        thresh = 2\n",
    "        maxValue = 2\n",
    "        ret, th1 = cv2.threshold(fgmask, thresh, maxValue, cv2.THRESH_BINARY)\n",
    "        # for testing purposes, show the threshold image\n",
    "        # cv2.imwrite('diff-th1.jpg', th1)\n",
    "\n",
    "        # add to the accumulated image\n",
    "        accum_image = cv2.add(accum_image, th1)\n",
    "        # for testing purposes, show the accumulated image\n",
    "        # cv2.imwrite('diff-accum.jpg', accum_image)\n",
    "\n",
    "        # for testing purposes, control frame by frame\n",
    "        # raw_input(\"press any key to continue\")\n",
    "\n",
    "    # for testing purposes, show the current frame\n",
    "    # cv2.imshow('frame', gray)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# apply a color map\n",
    "# COLORMAP_PINK also works well, COLORMAP_BONE is acceptable if the background is dark\n",
    "color_image = im_color = cv2.applyColorMap(accum_image, cv2.COLORMAP_HOT)\n",
    "# for testing purposes, show the colorMap image\n",
    "# cv2.imwrite('diff-color.jpg', color_image)\n",
    "\n",
    "# overlay the color mapped image to the first frame\n",
    "result_overlay = cv2.addWeighted(first_frame, 0.7, color_image, 0.7, 0)\n",
    "\n",
    "# save the final overlay image\n",
    "cv2.imwrite('diff-overlay.jpg', result_overlay)\n",
    "\n",
    "# cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T21:28:44.314371800Z",
     "start_time": "2023-06-20T21:28:28.235183700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "cap.release()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T21:26:39.236534500Z",
     "start_time": "2023-06-20T21:26:38.829246100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apro la webcam e elaboro al volo i frame\n",
    "\n",
    "webcam=cv2.VideoCapture(0)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura della webcam\")\n",
    "\n",
    "resize = transforms.Resize([224, 224])\n",
    "to_tensor = transforms.ToTensor()\n",
    "ret,frame=webcam.read()\n",
    "while ret:\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    #-----------------------------PREPROCESSING START HERE-----------------------------\n",
    "    #RED FILTERING\n",
    "    full_mask = red_filtering(frame)\n",
    "    #SEGMENTATION E CROPPING\n",
    "    # TODO: migliorare il meccanismo di cropping se deve tagliare pi√π di 224 meglio skippare il frame(creerebbe frame rettangolari e andrebbero in input alla rete immagini distorte)\n",
    "    cropped_image = segmentation_and_cropping(frame, full_mask)\n",
    "    #NORMALIZATION\n",
    "    normalized_image = cv2.normalize(cropped_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    #EQUALIZATION\n",
    "    equalized_image = equalizing(normalized_image)\n",
    "    #------------------------------PREPROCESSING END HERE------------------------------\n",
    "    img = cv2.cvtColor(equalized_image, cv2.COLOR_BGR2RGB)\n",
    "    img = squaring(img)\n",
    "\n",
    "    try:\n",
    "        # Pu√≤ dare problemi se gli arriva una immagine con una dimensione mancante,\n",
    "        # non ho capito il perch√® (probabilmente √® causata dall'algoritmo di cropping)\n",
    "        # ma questo pu√≤ succedere\n",
    "        im_pil = Image.fromarray(img)\n",
    "    except:\n",
    "        print(\"Shape dell'immagine che farebbe crashare il processo: \" + str(img.shape))\n",
    "        pass #TODO: controllare come va con il continue(per skippare un ciclo in caso di cropping fatto male)\n",
    "    frame = resize(im_pil)\n",
    "    tensor = to_tensor(frame)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if ret==True:\n",
    "        output, keypoints = inference(model, tensor)\n",
    "        restricted_keypoints = [[keypoints[0][0], keypoints[0][1], keypoints[0][6], keypoints[0][7], keypoints[0][12],keypoints[0][13]]]\n",
    "        embedding = assocEmbedding(keypoints)\n",
    "        #frame = drawSkeleton(tensor[0], embedding[0])\n",
    "        frame = drawKeypoints(tensor[0], restricted_keypoints[0])\n",
    "        cv2.imshow(\"Pose estimation\", frame)\n",
    "\n",
    "        #-----------------------------HOMOGRAPHY START HERE-----------------------------\n",
    "        # TODO: inserire treshold sul reprojection error(stavo pensando di impostarla a 0.1)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image_pil = Image.fromarray(frame, 'RGB')\n",
    "        image_pil.save('src/referee_gloves_detector/resources/src.jpg')\n",
    "        src = frame\n",
    "        dst = cv2.imread(\"src/referee_gloves_detector/resources/dst.jpg\")\n",
    "        h = Homography(src,dst)\n",
    "        punti2d = [[412, 271], [226, 271], [363, 481], [264, 481], [317, 191], [317, 266]]\n",
    "        punti3d = []\n",
    "        index_list = [0, 1, 6, 7, 12, 13]\n",
    "        count = 0\n",
    "        index_to_remove = []\n",
    "        for i in index_list:\n",
    "            try:\n",
    "                punti3d.append([keypoints[0][i][0]['x'], keypoints[0][i][0]['y']])\n",
    "            except:\n",
    "                index_to_remove.append(count)\n",
    "            finally:\n",
    "                count = count + 1\n",
    "        for index in sorted(index_to_remove, reverse=True):\n",
    "            del punti2d[index]\n",
    "        corr = h.normalize_points(punti2d,punti3d)\n",
    "        h._compute_view_based_homography(corr)\n",
    "        plan_view = cv2.warpPerspective(src, h.H, (dst.shape[1], dst.shape[0]))\n",
    "        plan_view = cv2.cvtColor(plan_view, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imshow(\"Pose estimation homography\", plan_view)\n",
    "        #------------------------------HOMOGRAPHY END HERE------------------------------\n",
    "\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break\n",
    "    ret, frame = webcam.read()\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
