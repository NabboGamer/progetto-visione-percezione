{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ce0c0a",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-17T23:36:45.842653800Z",
     "start_time": "2023-06-17T23:36:42.183648400Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(19, 'litepose-pose-estimation/src')\n",
    "sys.path.insert(20, 'src/referee_gloves_detector')\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import lp_config.lp_common_config as cc\n",
    "from lp_model.lp_litepose import LitePose\n",
    "from lp_inference.lp_inference import inference, assocEmbedding\n",
    "from lp_utils.lp_image_processing import drawHeatmap, drawKeypoints, drawSkeleton\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import red_filtering, segmentation_and_cropping, equalizing, squaring\n",
    "import math\n",
    "from Homography import Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f0f10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T23:36:45.859648100Z",
     "start_time": "2023-06-17T23:36:45.845648200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Leggo il file di configurazione\n",
    "with open('config/config.json') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Ottengo i percorsi dei file\n",
    "file_path_big_arch = config_data['path_big_arch']\n",
    "file_path_csv_keypoints = config_data['path_csv_keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3e6c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T23:36:46.113648700Z",
     "start_time": "2023-06-17T23:36:45.861668100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitePose().to(cc.config[\"device\"])\n",
    "model.load_state_dict(torch.load(file_path_big_arch, map_location=cc.config[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d724f808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T23:36:46.156646400Z",
     "start_time": "2023-06-17T23:36:46.115648500Z"
    }
   },
   "outputs": [],
   "source": [
    "def keypoints_saver(restricted_keypoints):\n",
    "    d = {'x': [], 'y': [], 'tag':[]}\n",
    "    df_keypoints = pd.DataFrame(data=d)\n",
    "    for i in range(len(restricted_keypoints[0])):\n",
    "        if len(restricted_keypoints[0][i])==0:\n",
    "            df_temp = pd.DataFrame({'x':np.nan, 'y':np.nan, 'tag':np.nan}, index=[i])\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "        else:\n",
    "            print(restricted_keypoints[0][i][0])\n",
    "            df_temp = pd.DataFrame(restricted_keypoints[0][i][0], index=[i])\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "\n",
    "    df_keypoints.to_csv(file_path_csv_keypoints, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keypoints_saver() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m im_pil \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img)\n\u001B[0;32m     16\u001B[0m im_pil\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msrc/referee_gloves_detector/resources/src.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 17\u001B[0m \u001B[43mkeypoints_saver\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m01/01/1970 01:00:00\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mkeypoints\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m src \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc/referee_gloves_detector/resources/src.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m dst \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc/referee_gloves_detector/resources/dst.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: keypoints_saver() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# HOMOGRAPHY IMAGE\n",
    "\n",
    "uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\resources\\\\bloisi_nao_red_hands.png\"\n",
    "img = Image.open(uri).convert('RGB')\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "to_tensor = transforms.ToTensor()\n",
    "tensor = to_tensor(img)\n",
    "tensor = tensor.unsqueeze(0)\n",
    "output, keypoints = inference(model, tensor)\n",
    "embedding = assocEmbedding(keypoints)\n",
    "idx = 0\n",
    "img = drawKeypoints(tensor[idx], keypoints[idx])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img)\n",
    "im_pil.save('src/referee_gloves_detector/resources/src.jpg')\n",
    "keypoints_saver(['01/01/1970 01:00:00'], [keypoints])\n",
    "src = cv2.imread(\"src/referee_gloves_detector/resources/src.jpg\")\n",
    "dst = cv2.imread(\"src/referee_gloves_detector/resources/dst.jpg\")\n",
    "h = Homography(src,dst)\n",
    "h._from_detection()\n",
    "plan_view = cv2.warpPerspective(src, h.H, (dst.shape[1], dst.shape[0]))\n",
    "cv2.imshow(\"Reprojected view\", plan_view)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T23:37:13.916008800Z",
     "start_time": "2023-06-17T23:37:12.733977600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 125, 'y': 45, 'tag': 0.45596110820770264}\n",
      "{'x': 65, 'y': 50, 'tag': 0.3447529077529907}\n",
      "{'x': 116, 'y': 113, 'tag': 0.39876893162727356}\n",
      "{'x': 93, 'y': 113, 'tag': 0.4468965232372284}\n",
      "{'x': 129, 'y': 162, 'tag': 0.3563166558742523}\n",
      "{'x': 86, 'y': 159, 'tag': 0.37395671010017395}\n",
      "{'x': 146, 'y': 202, 'tag': 0.4609355628490448}\n",
      "{'x': 93, 'y': 200, 'tag': 0.4463735818862915}\n",
      "{'x': 91, 'y': 5, 'tag': 0.39669370651245117}\n",
      "{'x': 94, 'y': 38, 'tag': 0.45942795276641846}\n",
      "125.0\n",
      "45.0\n",
      "65.0\n",
      "50.0\n",
      "116.0\n",
      "113.0\n",
      "93.0\n",
      "113.0\n",
      "129.0\n",
      "162.0\n",
      "86.0\n",
      "159.0\n",
      "146.0\n",
      "202.0\n",
      "93.0\n",
      "200.0\n",
      "91.0\n",
      "5.0\n",
      "94.0\n",
      "38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stolf\\anaconda3\\envs\\visione-percezione-lite\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65, 50], [86, 159], [91, 5], [93, 113], [93, 200], [94, 38], [116, 113], [125, 45], [129, 162], [146, 202]]\n",
      "[0 2 0 2 1 0 2 0 1 1]\n",
      "CLUSTER N.  0\n",
      "   Group  Points_x  Points_y\n",
      "2      0        91         5\n",
      "5      0        94        38\n",
      "7      0       125        45\n",
      "0      0        65        50\n",
      "CLUSTER N.  1\n",
      "   Group  Points_x  Points_y\n",
      "8      1       129       162\n",
      "4      1        93       200\n",
      "9      1       146       202\n",
      "CLUSTER N.  2\n",
      "   Group  Points_x  Points_y\n",
      "3      2        93       113\n",
      "6      2       116       113\n",
      "1      2        86       159\n",
      "[1 0 2 2 1 1 2 0 0 1]\n",
      "Matrix: A : meanx 103.8, meany 108.7, varx 532.9600000000002, vary 4570.409999999999, sx 0.06125868786374019, sy 0.02091883128172933 \n",
      "Matrix: B : meanx 165.5, meany 297.7, varx 3888.05, vary 30671.01, sx 0.022680314746437415, sy 0.008075156707140992 \n",
      "Numero punti campo reale: 10\n",
      "Numero punti campo virtuale: 10\n",
      "Number of points in current view :  10\n",
      "Shape of Matrix M :  (20, 9)\n",
      "N_model\n",
      " [[ 0.06125869  0.         -6.3586518 ]\n",
      " [ 0.          0.02091883 -2.27387696]\n",
      " [ 0.          0.          1.        ]]\n",
      "N_observed\n",
      " [[ 0.02268031  0.         -3.75359209]\n",
      " [ 0.          0.00807516 -2.40397415]\n",
      " [ 0.          0.          1.        ]]\n",
      "p_model (-0.7841112046558738, -2.1692828039153316) \t p_obs (1.4401999863987762, 1.0764183890618941)\n",
      "p_model (-0.6003351410646536, -1.4789613716182637) \t p_obs (-1.485560615891651, 1.8677837463617117)\n",
      "p_model (1.298684182711293, -1.3325295526461582) \t p_obs (1.4175196716523382, 1.8677837463617117)\n",
      "p_model (-2.3768370891131188, -1.2279353962375117) \t p_obs (0.011340157373218585, -2.0728927267230923)\n",
      "p_model (1.543718934166253, 1.1149737073161732) \t p_obs (0.011340157373218585, -1.4834062871018001)\n",
      "p_model (-0.6615938289283934, 1.9098892960218876) \t p_obs (-2.0752487992990236, -1.426880190151813)\n",
      "p_model (2.585116627849837, 1.951726958585346) \t p_obs (2.211330687777648, -1.426880190151813)\n",
      "p_model (-0.6615938289283934, 0.08995097451143597) \t p_obs (-1.1226755799486523, 0.26082756164065435)\n",
      "p_model (0.7473559919376305, 0.08995097451143597) \t p_obs (1.05463463570934, 0.26082756164065435)\n",
      "p_model (-1.0904046439745745, 1.0522172134709855) \t p_obs (-1.4628803011452134, 1.0764183890618941)\n",
      "Computing SVD of M\n",
      "[[ 44.09109887   0.         165.5       ]\n",
      " [  0.         123.83660606 297.7       ]\n",
      " [  0.           0.           1.        ]]\n",
      "[[ 0.06125869  0.         -6.3586518 ]\n",
      " [ 0.          0.02091883 -2.27387696]\n",
      " [ 0.          0.          1.        ]]\n",
      "Homography for View : \n",
      " [[-3.89162287e+00  4.78840022e-01  2.66133196e+02]\n",
      " [-4.67366582e+00  1.86813245e+00  1.60615186e+02]\n",
      " [-1.48681209e-02  1.88608064e-03  1.00000000e+00]]\n",
      "Imp [229, 431] | ObjP [91, 5] | Tx [[249.17959787 743.22200738   1.        ]]\n",
      "Imp [100, 529] | ObjP [94, 38] | Tx [[250.00109859 637.31139848   1.        ]]\n",
      "Imp [228, 529] | ObjP [125, 45] | Tx [[256.93020119 438.86876616   1.        ]]\n",
      "Imp [166, 41] | ObjP [65, 50] | Tx [[ 290.2785514 -389.1770354    1.       ]]\n",
      "Imp [166, 114] | ObjP [129, 162] | Tx [[258.49620578 228.02180201   1.        ]]\n",
      "Imp [74, 121] | ObjP [93, 200] | Tx [[ 3.57416451e+00 -1.80446985e+04  1.00000000e+00]]\n",
      "Imp [263, 121] | ObjP [146, 202] | Tx [[259.97612516 182.81218749   1.        ]]\n",
      "Imp [116, 330] | ObjP [93, 113] | Tx [[245.73591023 371.07165603   1.        ]]\n",
      "Imp [212, 330] | ObjP [116, 113] | Tx [[256.43582504 333.14980896   1.        ]]\n",
      "Imp [101, 431] | ObjP [86, 159] | Tx [[3.57501448e+02 2.62445197e+03 1.00000000e+00]]\n",
      "Reprojection error :  0.47370936375544176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stolf\\anaconda3\\envs\\visione-percezione-lite\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# HOMOGRAPHY IMAGE AUTOMATICALLY\n",
    "\n",
    "uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\resources\\\\bloisi_nao_red_hands.png\"\n",
    "img = Image.open(uri).convert('RGB')\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "to_tensor = transforms.ToTensor()\n",
    "tensor = to_tensor(img)\n",
    "tensor = tensor.unsqueeze(0)\n",
    "output, keypoints = inference(model, tensor)\n",
    "restricted_keypoints = [[keypoints[0][0], keypoints[0][1], keypoints[0][6], keypoints[0][7], keypoints[0][8], keypoints[0][9], keypoints[0][10],keypoints[0][11],keypoints[0][12],keypoints[0][13]]]\n",
    "embedding = assocEmbedding(keypoints)\n",
    "idx = 0\n",
    "img = drawKeypoints(tensor[idx], restricted_keypoints[idx])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img)\n",
    "im_pil.save('src/referee_gloves_detector/resources/src.jpg')\n",
    "keypoints_saver(restricted_keypoints)\n",
    "src = cv2.imread(\"src/referee_gloves_detector/resources/src.jpg\")\n",
    "dst = cv2.imread(\"src/referee_gloves_detector/resources/dst.jpg\")\n",
    "h = Homography(src,dst)\n",
    "h._from_detection_automatically()\n",
    "plan_view = cv2.warpPerspective(src, h.H, (dst.shape[1], dst.shape[0]))\n",
    "cv2.imshow(\"Reprojected view\", plan_view)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T23:38:04.157827500Z",
     "start_time": "2023-06-17T23:37:22.340384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HOMOGRAPHY VIDEO\n",
    "\n",
    "uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\resources\\\\referee_1.mp4\"\n",
    "cap = cv2.VideoCapture(uri)\n",
    "\n",
    "# Verifico se il video è stato aperto correttamente\n",
    "if not cap.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura del video\")\n",
    "\n",
    "keypoints_vec = []\n",
    "timestamps = []\n",
    "resize = transforms.Resize([224, 224])\n",
    "to_tensor = transforms.ToTensor()\n",
    "ret,frame=cap.read()\n",
    "count = 0\n",
    "while ret:\n",
    "\n",
    "    istante_attuale = datetime.now()\n",
    "    stringa_istante = istante_attuale.strftime(\"%d/%m/%Y %H:%M:%S.%f\")\n",
    "    timestamps.append(stringa_istante)\n",
    "\n",
    "    #-----------------------------PREPROCESSING START HERE-----------------------------\n",
    "    #NORMALIZATION\n",
    "    normalized_image = cv2.normalize(frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    #EQUALIZATION\n",
    "    equalized_image = equalizing(normalized_image)\n",
    "    #------------------------------PREPROCESSING END HERE------------------------------\n",
    "    img = cv2.cvtColor(equalized_image, cv2.COLOR_BGR2RGB)\n",
    "    img = squaring(img)\n",
    "\n",
    "    try:\n",
    "        # Può dare problemi se gli arriva una immagine con una dimensione mancante,\n",
    "        # non ho capito il perchè (probabilmente è causata dall'algoritmo di cropping)\n",
    "        # ma questo può succedere\n",
    "        im_pil = Image.fromarray(img)\n",
    "    except:\n",
    "        print(\"Shape dell'immagine che farebbe crashare il processo: \" + str(img.shape))\n",
    "        pass\n",
    "    frame = resize(im_pil)\n",
    "    tensor = to_tensor(frame)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if ret==True:\n",
    "        output, keypoints = inference(model, tensor)\n",
    "        keypoints_vec.append(keypoints)\n",
    "        embedding = assocEmbedding(keypoints)\n",
    "        #frame_modified = drawSkeleton(tensor[0], embedding[0])\n",
    "        frame = drawKeypoints(tensor[0], keypoints[0])\n",
    "        if count == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_pil = Image.fromarray(frame, 'RGB')\n",
    "            image_pil.save('src/referee_gloves_detector/resources/src.jpg')\n",
    "            keypoints_saver(timestamps, keypoints_vec)\n",
    "            src = cv2.imread(\"src/referee_gloves_detector/resources/src.jpg\")\n",
    "            dst = cv2.imread(\"src/referee_gloves_detector/resources/dst.jpg\")\n",
    "            h = Homography(src,dst)\n",
    "            h._from_detection()\n",
    "            count = count + 1\n",
    "        frame = cv2.warpPerspective(frame, h.H, (dst.shape[1], dst.shape[0]))\n",
    "        cv2.imshow(\"Pose estimation\", frame)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Dimensioni dell'array di frame:\", len(keypoints_vec))\n",
    "print(\"Dimensioni dell'array di timestamp:\", len(timestamps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apro la webcam e salvo al volo i frame e il timestamp\n",
    "\n",
    "webcam=cv2.VideoCapture(0)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura della webcam\")\n",
    "\n",
    "keypoints_vec = []\n",
    "timestamps = []\n",
    "resize = transforms.Resize([224, 224])\n",
    "to_tensor = transforms.ToTensor()\n",
    "ret,frame=webcam.read()\n",
    "while ret:\n",
    "\n",
    "\n",
    "    istante_attuale = datetime.now()\n",
    "    stringa_istante = istante_attuale.strftime(\"%d/%m/%Y %H:%M:%S.%f\")\n",
    "    timestamps.append(stringa_istante)\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    #-----------------------------PREPROCESSING START HERE-----------------------------\n",
    "    #RED FILTERING\n",
    "    full_mask = red_filtering(frame)\n",
    "    #SEGMENTATION E CROPPING\n",
    "    cropped_image = segmentation_and_cropping(frame, full_mask)\n",
    "    #NORMALIZATION\n",
    "    normalized_image = cv2.normalize(cropped_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    #EQUALIZATION\n",
    "    equalized_image = equalizing(normalized_image)\n",
    "    #------------------------------PREPROCESSING END HERE------------------------------\n",
    "    img = cv2.cvtColor(equalized_image, cv2.COLOR_BGR2RGB)\n",
    "    img = squaring(img)\n",
    "\n",
    "    try:\n",
    "        # Può dare problemi se gli arriva una immagine con una dimensione mancante,\n",
    "        # non ho capito il perchè (probabilmente è causata dall'algoritmo di cropping)\n",
    "        # ma questo può succedere\n",
    "        im_pil = Image.fromarray(img)\n",
    "    except:\n",
    "        print(\"Shape dell'immagine che farebbe crashare il processo: \" + str(img.shape))\n",
    "        pass\n",
    "    frame = resize(im_pil)\n",
    "    tensor = to_tensor(frame)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if ret==True:\n",
    "        output, keypoints = inference(model, tensor)\n",
    "        keypoints_vec.append(keypoints)\n",
    "        embedding = assocEmbedding(keypoints)\n",
    "        frame_modified = drawSkeleton(tensor[0], embedding[0])\n",
    "        #frame = drawKeypoints(tensor[0], keypoints[0])\n",
    "        cv2.imshow(\"Pose estimation\", frame_modified)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break\n",
    "    ret, frame = webcam.read()\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Dimensioni dell'array di frame:\", len(keypoints_vec))\n",
    "print(\"Dimensioni dell'array di timestamp:\", len(timestamps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbbadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
