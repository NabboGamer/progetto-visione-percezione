{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce0c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(19, 'litepose-pose-estimation/src')\n",
    "sys.path.insert(20, 'src/referee_gloves_detector')\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import lp_config.lp_common_config as cc\n",
    "from lp_model.lp_litepose import LitePose\n",
    "from lp_inference.lp_inference import inference, assocEmbedding\n",
    "from lp_utils.lp_image_processing import drawHeatmap, drawKeypoints, drawSkeleton\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import red_filtering, segmentation_and_cropping, equalizing, squaring\n",
    "import math\n",
    "from Homography import Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggo il file di configurazione\n",
    "with open('config/config.json') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Ottengo i percorsi dei file\n",
    "file_path_big_arch = config_data['path_big_arch']\n",
    "file_path_csv_keypoints = config_data['path_csv_keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitePose().to(cc.config[\"device\"])\n",
    "model.load_state_dict(torch.load(file_path_big_arch, map_location=cc.config[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_saver(timestamps, keypoints_vec):\n",
    "    d = {'timestamp':[],'x': [], 'y': [], 'tag':[]}\n",
    "    df_keypoints = pd.DataFrame(data=d)\n",
    "    for timestamp, restricted_keypoints in zip(timestamps, keypoints_vec):\n",
    "        for i in range(len(restricted_keypoints[0])):\n",
    "            if not (restricted_keypoints[0][i]):\n",
    "                df_temp = pd.DataFrame([{'timestamp': '01/01/1970 01:00:00','x':np.nan, 'y':np.nan, 'tag':np.nan}])\n",
    "                df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "            else:\n",
    "                df_temp = pd.DataFrame(restricted_keypoints[0][i])\n",
    "                df_temp['timestamp'] = timestamp\n",
    "                df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "    df_keypoints.to_csv(file_path_csv_keypoints, index=False)\n",
    "    df_multiindex = df_keypoints.set_index(['timestamp'])\n",
    "    #display(df_multiindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HOMOGRAPHY IMAGE\n",
    "\n",
    "uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\resources\\\\bloisi_nao_red_hands.png\"\n",
    "img = Image.open(uri).convert('RGB')\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "to_tensor = transforms.ToTensor()\n",
    "tensor = to_tensor(img)\n",
    "tensor = tensor.unsqueeze(0)\n",
    "output, keypoints = inference(model, tensor)\n",
    "embedding = assocEmbedding(keypoints)\n",
    "idx = 0\n",
    "img = drawKeypoints(tensor[idx], keypoints[idx])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img)\n",
    "im_pil.save('src/referee_gloves_detector/resources/src.jpg')\n",
    "keypoints_saver(['01/01/1970 01:00:00'], [keypoints])\n",
    "src = cv2.imread(\"src/referee_gloves_detector/resources/src.jpg\")\n",
    "dst = cv2.imread(\"src/referee_gloves_detector/resources/dst.jpg\")\n",
    "h = Homography(src,dst)\n",
    "h._from_detection()\n",
    "plan_view = cv2.warpPerspective(src, h.H, (dst.shape[1], dst.shape[0]))\n",
    "cv2.imshow(\"Reprojected view\", plan_view)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HOMOGRAPHY VIDEO\n",
    "\n",
    "uri = \"C:\\\\Users\\\\stolf\\\\dev\\\\Progetto Visione e Percezione\\\\src\\\\referee_gloves_detector\\\\resources\\\\referee_1.mp4\"\n",
    "cap = cv2.VideoCapture(uri)\n",
    "\n",
    "# Verifico se il video è stato aperto correttamente\n",
    "if not cap.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura del video\")\n",
    "\n",
    "keypoints_vec = []\n",
    "timestamps = []\n",
    "resize = transforms.Resize([224, 224])\n",
    "to_tensor = transforms.ToTensor()\n",
    "ret,frame=cap.read()\n",
    "count = 0\n",
    "while ret:\n",
    "\n",
    "    istante_attuale = datetime.now()\n",
    "    stringa_istante = istante_attuale.strftime(\"%d/%m/%Y %H:%M:%S.%f\")\n",
    "    timestamps.append(stringa_istante)\n",
    "\n",
    "    #-----------------------------PREPROCESSING START HERE-----------------------------\n",
    "    #NORMALIZATION\n",
    "    normalized_image = cv2.normalize(frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    #EQUALIZATION\n",
    "    equalized_image = equalizing(normalized_image)\n",
    "    #------------------------------PREPROCESSING END HERE------------------------------\n",
    "    img = cv2.cvtColor(equalized_image, cv2.COLOR_BGR2RGB)\n",
    "    img = squaring(img)\n",
    "\n",
    "    try:\n",
    "        # Può dare problemi se gli arriva una immagine con una dimensione mancante,\n",
    "        # non ho capito il perchè (probabilmente è causata dall'algoritmo di cropping)\n",
    "        # ma questo può succedere\n",
    "        im_pil = Image.fromarray(img)\n",
    "    except:\n",
    "        print(\"Shape dell'immagine che farebbe crashare il processo: \" + str(img.shape))\n",
    "        pass\n",
    "    frame = resize(im_pil)\n",
    "    tensor = to_tensor(frame)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if ret==True:\n",
    "        output, keypoints = inference(model, tensor)\n",
    "        keypoints_vec.append(keypoints)\n",
    "        embedding = assocEmbedding(keypoints)\n",
    "        #frame_modified = drawSkeleton(tensor[0], embedding[0])\n",
    "        frame = drawKeypoints(tensor[0], keypoints[0])\n",
    "        if count == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_pil = Image.fromarray(frame, 'RGB')\n",
    "            image_pil.save('src/referee_gloves_detector/resources/src.jpg')\n",
    "            keypoints_saver(timestamps, keypoints_vec)\n",
    "            src = cv2.imread(\"src/referee_gloves_detector/resources/src.jpg\")\n",
    "            dst = cv2.imread(\"src/referee_gloves_detector/resources/dst.jpg\")\n",
    "            h = Homography(src,dst)\n",
    "            h._from_detection()\n",
    "            count = count + 1\n",
    "        frame = cv2.warpPerspective(frame, h.H, (dst.shape[1], dst.shape[0]))\n",
    "        cv2.imshow(\"Pose estimation\", frame)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Dimensioni dell'array di frame:\", len(keypoints_vec))\n",
    "print(\"Dimensioni dell'array di timestamp:\", len(timestamps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apro la webcam e salvo al volo i frame e il timestamp\n",
    "\n",
    "webcam=cv2.VideoCapture(0)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura della webcam\")\n",
    "\n",
    "keypoints_vec = []\n",
    "timestamps = []\n",
    "resize = transforms.Resize([224, 224])\n",
    "to_tensor = transforms.ToTensor()\n",
    "ret,frame=webcam.read()\n",
    "while ret:\n",
    "\n",
    "\n",
    "    istante_attuale = datetime.now()\n",
    "    stringa_istante = istante_attuale.strftime(\"%d/%m/%Y %H:%M:%S.%f\")\n",
    "    timestamps.append(stringa_istante)\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    #-----------------------------PREPROCESSING START HERE-----------------------------\n",
    "    #RED FILTERING\n",
    "    full_mask = red_filtering(frame)\n",
    "    #SEGMENTATION E CROPPING\n",
    "    cropped_image = segmentation_and_cropping(frame, full_mask)\n",
    "    #NORMALIZATION\n",
    "    normalized_image = cv2.normalize(cropped_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    #EQUALIZATION\n",
    "    equalized_image = equalizing(normalized_image)\n",
    "    #------------------------------PREPROCESSING END HERE------------------------------\n",
    "    img = cv2.cvtColor(equalized_image, cv2.COLOR_BGR2RGB)\n",
    "    img = squaring(img)\n",
    "\n",
    "    try:\n",
    "        # Può dare problemi se gli arriva una immagine con una dimensione mancante,\n",
    "        # non ho capito il perchè (probabilmente è causata dall'algoritmo di cropping)\n",
    "        # ma questo può succedere\n",
    "        im_pil = Image.fromarray(img)\n",
    "    except:\n",
    "        print(\"Shape dell'immagine che farebbe crashare il processo: \" + str(img.shape))\n",
    "        pass\n",
    "    frame = resize(im_pil)\n",
    "    tensor = to_tensor(frame)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if ret==True:\n",
    "        output, keypoints = inference(model, tensor)\n",
    "        keypoints_vec.append(keypoints)\n",
    "        embedding = assocEmbedding(keypoints)\n",
    "        frame_modified = drawSkeleton(tensor[0], embedding[0])\n",
    "        #frame = drawKeypoints(tensor[0], keypoints[0])\n",
    "        cv2.imshow(\"Pose estimation\", frame_modified)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break\n",
    "    ret, frame = webcam.read()\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Dimensioni dell'array di frame:\", len(keypoints_vec))\n",
    "print(\"Dimensioni dell'array di timestamp:\", len(timestamps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbbadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
