{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b839e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "sys.path.insert(19, 'litepose-pose-estimation/src')\n",
    "\n",
    "from lp_coco_utils.lp_getDataset import getDatasetProcessed\n",
    "from lp_training.lp_trainer import train\n",
    "from lp_model.lp_litepose import LitePose\n",
    "from lp_inference.lp_inference import inference, assocEmbedding\n",
    "from lp_utils.lp_image_processing import drawHeatmap, drawKeypoints, drawSkeleton\n",
    "from lp_testing.lp_evaluate import evaluateModel\n",
    "\n",
    "import lp_config.lp_common_config as cc\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import av\n",
    "import streamlink\n",
    "\n",
    "from pytube import YouTube\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1679d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggo il file di configurazione\n",
    "with open('config/config.json') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Ottengo i percorsi dei file\n",
    "file_path_big_arch = config_data['path_big_arch']\n",
    "\n",
    "file_path_video = config_data['path_video']\n",
    "\n",
    "file_path_csv_keypoints = config_data['path_csv_keypoints']\n",
    "file_path_csv_keypoints_video = config_data['path_csv_keypoints_video'] \n",
    "file_path_csv_keypoints_webcam = config_data['path_csv_keypoints_webcam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeff6c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carico il modello già addestrato\n",
    "\n",
    "model = LitePose().to(cc.config[\"device\"])\n",
    "model.load_state_dict(torch.load(file_path_big_arch, map_location=cc.config[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico una immagine statica da url\n",
    "\n",
    "url = 'https://web.unibas.it/bloisi/assets/images/bloisi_nao.jpg'\n",
    "#url = 'https://previews.123rf.com/images/mimagephotography/mimagephotography1411/mimagephotography141100024/33214727-full-body-portrait-of-a-handsome-young-african-american-man-smiling-on-isolated-white-background.jpg'\n",
    "\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\Screenshot (113).png\"\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\Screenshot (114).png\"\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\Screenshot (115).png\"\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\Screenshot (116).png\"\n",
    "#uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\Screenshot (117).png\"\n",
    "# uri = \"C:\\\\Users\\\\stolf\\\\Downloads\\\\Screenshot (118).png\"\n",
    "\n",
    "img = Image.open(urlopen(url)).convert('RGB')\n",
    "frame = np.array(img)\n",
    "frame = frame[]\n",
    "cv2.imshow(\"Array\",frame)\n",
    "cv2.waitKey(90000)\n",
    "cv2.destroyAllWindows()\n",
    "# img = Image.open(uri).convert('RGB')\n",
    "plt.grid(False)\n",
    "_= plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64181517",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "ret,frame = cap.read()\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dab6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adatto l'immagine alle dimensioni di un tensore PyTorch \n",
    "\n",
    "# Ridimensiono l'immagine\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Trasformo l'immagine in un tensore\n",
    "tensor = to_tensor(img)\n",
    "# torch.Size([3, 224, 224])\n",
    "\n",
    "# Aggiungo un'altra dimensione per far corrispondere la shape del tensore alla shape (NCHW) di un tensore PyTorch\n",
    "tensor = tensor.unsqueeze(0)\n",
    "# torch.Size([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferisco i Keypoints dall'immagine\n",
    "\n",
    "output, keypoints = inference(model, tensor)\n",
    "embedding = assocEmbedding(keypoints)\n",
    "restricted_keypoints = [[keypoints[0][0], keypoints[0][1], keypoints[0][2], keypoints[0][3], keypoints[0][4], keypoints[0][5], keypoints[0][6],keypoints[0][7],keypoints[0][13]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegno i keypoints inferiti sull'immagine\n",
    "\n",
    "idx = 0\n",
    "img = drawKeypoints(tensor[idx], restricted_keypoints[idx])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img)\n",
    "plt.grid(False)\n",
    "_= plt.imshow(im_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ceeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegno lo skeleton a partire dai keypoints\n",
    "\n",
    "idx = 0\n",
    "img = drawSkeleton(tensor[idx], embedding[idx])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img)\n",
    "plt.grid(False)\n",
    "_= plt.imshow(im_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo solo i 9 keypoints scelti\n",
    "\n",
    "#display(restricted_keypoints)\n",
    "d = {'x': [], 'y': [], 'tag':[]}\n",
    "df_keypoints = pd.DataFrame(data=d)\n",
    "for i in range(len(restricted_keypoints[0])):\n",
    "    if not (restricted_keypoints[0][i]):\n",
    "        df_temp = pd.DataFrame([{'x':np.nan, 'y':np.nan, 'tag':np.nan}])\n",
    "        df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "    else:\n",
    "        df_temp = pd.DataFrame(restricted_keypoints[0][i])\n",
    "        df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "df_keypoints.reset_index(drop=True, inplace=True)\n",
    "df_keypoints.to_csv(file_path_csv_keypoints, index=False)\n",
    "#df_keypoints.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico una video da url\n",
    "\n",
    "def video_downloader(link):\n",
    "    youtubeObject = YouTube(link)\n",
    "    youtubeObject = youtubeObject.streams.get_by_resolution(\"360p\")\n",
    "    try:\n",
    "        youtubeObject.download(filename='referee.mp4')\n",
    "    except:\n",
    "        raise Exception(\"Si è verificato un errore nel download del video\")\n",
    "    print(\"Download completato correttamente\")\n",
    "\n",
    "\n",
    "url = 'https://youtu.be/bBuDwm6JrZw'\n",
    "video_downloader(url)\n",
    "\n",
    "# Apro il file video\n",
    "cap = cv2.VideoCapture(file_path_video)\n",
    "\n",
    "# Verifico se il video è stato aperto correttamente\n",
    "if not cap.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura del video\")\n",
    "\n",
    "frames = []\n",
    "timestamps = []\n",
    "ret, frame = cap.read()\n",
    "count = 0\n",
    "while ret:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    im_pil = Image.fromarray(img)\n",
    "    frames.append(im_pil)\n",
    "    # Ottengo l'istante attuale\n",
    "    istante_attuale = datetime.now()\n",
    "    # Converto l'istante in una stringa nel formato desiderato\n",
    "    stringa_istante = istante_attuale.strftime(\"%d/%m/%Y %H:%M:%S.%f\")\n",
    "    timestamps.append(stringa_istante)\n",
    "    # Leggo il successivo frame\n",
    "    ret, frame = cap.read()\n",
    "    count += 1\n",
    "    # Imposto il frame successivo da leggere dopo 15 ms\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, count * 15)\n",
    "\n",
    "# Chiudo il file video\n",
    "cap.release()\n",
    "\n",
    "# Stampo le dimensioni dell'array\n",
    "print(\"Dimensioni dell'array di frame:\", len(frames))\n",
    "print(\"Dimensioni dell'array di timestamp:\", len(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diminuisco il numero di frame per alleggerire il carico computazionale\n",
    "\n",
    "frames = frames[0:500]\n",
    "timestamps = timestamps[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ec4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adatto le immagini alle dimensioni dei tensori PyTorch \n",
    "\n",
    "display(len(frames)) # numero di frames (immagini)\n",
    "\n",
    "# Adatto i frames alle dimensioni di un tensore PyTorch \n",
    "resize = transforms.Resize([224, 224])  \n",
    "to_tensor = transforms.ToTensor() \n",
    "tensors = [] # Creo la lista di tensori\n",
    "for frame in frames:\n",
    "    frame = resize(frame) # Ridimensiono le dimensioni dei frames\n",
    "    tensor = to_tensor(frame) # Trasformo il frame in tensore\n",
    "    tensor = tensor.unsqueeze(0) # Aggiungo un'altra dimensione per far corrispondere la shape del tensore alla shape (NCHW) di un tensore PyTorch\n",
    "    tensors.append(tensor) # Aggiungo il frame convertito in tensore alla lista di tensori\n",
    "\n",
    "display(len(tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c57b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferisco i Keypoints dai frame\n",
    "\n",
    "restricted_keypoints_vec = []\n",
    "embedding_vec = []\n",
    "for tensor in tqdm(tensors):\n",
    "    output, keypoints = inference(model, tensor)\n",
    "    embedding = assocEmbedding(keypoints)\n",
    "    restricted_keypoints = [[keypoints[0][0], keypoints[0][1], keypoints[0][2], keypoints[0][3], keypoints[0][4], keypoints[0][5], keypoints[0][6],keypoints[0][7],keypoints[0][13]]]    \n",
    "    restricted_keypoints_vec.append(restricted_keypoints)\n",
    "    embedding_vec.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0870c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Disegno i keypoints inferiti sui frame\n",
    "\n",
    "idx = 0\n",
    "for tensor, restricted_keypoints in zip(tensors, restricted_keypoints_vec):\n",
    "    frame = drawKeypoints(tensor[idx], restricted_keypoints[idx])\n",
    "    cv2.imshow(\"Image Keypoints\", frame)\n",
    "    cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegno gli skeleton a partire dai keypoints\n",
    "\n",
    "idx = 0\n",
    "for tensor, embedding in zip(tensors, embedding_vec):\n",
    "    frame = drawSkeleton(tensor[idx], embedding[idx])\n",
    "    cv2.imshow(\"Image Keypoints\", frame)\n",
    "    cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo solo i 9 keypoints scelti per ogni frame\n",
    "\n",
    "d = {'timestamp':[],'x': [], 'y': [], 'tag':[]}\n",
    "df_keypoints = pd.DataFrame(data=d)\n",
    "for timestamp, restricted_keypoints in zip(timestamps, restricted_keypoints_vec):\n",
    "    for i in range(len(restricted_keypoints[0])):\n",
    "        if not (restricted_keypoints[0][i]):\n",
    "            df_temp = pd.DataFrame([{'timestamp': '01/01/1970 01:00:00','x':np.nan, 'y':np.nan, 'tag':np.nan}])\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "        else:\n",
    "            df_temp = pd.DataFrame(restricted_keypoints[0][i])\n",
    "            df_temp['timestamp'] = timestamp\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "df_keypoints.to_csv(file_path_csv_keypoints_video, index=False)\n",
    "df_multiindex = df_keypoints.set_index(['timestamp'])\n",
    "#display(df_multiindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a089300",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     output, keypoints \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m assocEmbedding(keypoints)\n\u001b[0;32m     41\u001b[0m     frame \u001b[38;5;241m=\u001b[39m drawSkeleton(tensor[\u001b[38;5;241m0\u001b[39m], embedding[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\dev\\progetti\\VP\\progetto-visione-percezione\\litepose-pose-estimation/src\\lp_inference\\lp_inference.py:73\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, images)\u001b[0m\n\u001b[0;32m     71\u001b[0m hmavg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor((hm1\u001b[38;5;241m+\u001b[39mhm2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     72\u001b[0m hmavg \u001b[38;5;241m=\u001b[39m hmavg\u001b[38;5;241m/\u001b[39mhmavg\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m---> 73\u001b[0m hmavg \u001b[38;5;241m=\u001b[39m \u001b[43msuppression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhmavg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m tgavg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor((tg1\u001b[38;5;241m+\u001b[39mtg2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     76\u001b[0m tgavg \u001b[38;5;241m=\u001b[39m tgavg\u001b[38;5;241m/\u001b[39mtgavg\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\dev\\progetti\\VP\\progetto-visione-percezione\\litepose-pose-estimation/src\\lp_inference\\lp_inference.py:17\u001b[0m, in \u001b[0;36msuppression\u001b[1;34m(det)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuppression\u001b[39m(det):\n\u001b[0;32m     16\u001b[0m     pool \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMaxPool2d(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     maxm \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     maxm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meq(maxm, det)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     19\u001b[0m     det \u001b[38;5;241m=\u001b[39m det \u001b[38;5;241m*\u001b[39m maxm\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apro la webcam e salvo al volo i frame e il timestamp\n",
    "\n",
    "# Creo un oggetto di tipo webcam collegato alla webcam di default.\n",
    "# 0 indica la webcam predefinita, se sono presenti più webcam è possibile specificare il numero corrispondente\n",
    "webcam=cv2.VideoCapture(0) \n",
    "\n",
    "# Verifico se il video è stato aperto correttamente\n",
    "if not webcam.isOpened():\n",
    "    raise Exception(\"Errore nell'apertura della webcam\")\n",
    "\n",
    "frames = []\n",
    "timestamps = []\n",
    "resize = transforms.Resize([224, 224])  \n",
    "to_tensor = transforms.ToTensor()\n",
    "ret,frame=webcam.read()\n",
    "while ret:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = img[:,80:]\n",
    "    img = img[:,:-80]\n",
    "    im_pil = Image.fromarray(img)\n",
    "    frames.append(im_pil)\n",
    "    # Ottengo l'istante attuale\n",
    "    istante_attuale = datetime.now()\n",
    "    # Converto l'istante in una stringa nel formato desiderato\n",
    "    stringa_istante = istante_attuale.strftime(\"%d/%m/%Y %H:%M:%S.%f\")\n",
    "    timestamps.append(stringa_istante) \n",
    "    # Catturo l'input da tastiera,la funzione cv2.waitKey() attende per un determinato numero di millisecondi \n",
    "    # (nel caso specifico 15) e restituisce un valore intero che rappresenta il tasto premuto dall'utente.\n",
    "    # Il risultato viene assegnato alla variabile key. L'operatore & viene utilizzato per fare una maschera bit a bit \n",
    "    # con il valore esadecimale 0xFF, che rappresenta 255 in decimale. Ciò serve per estrarre solo gli ultimi 8 bit \n",
    "    # del valore restituito, ignorando eventuali bit aggiuntivi.\n",
    "    # Con if key==ord(\"q\") verifico se il tasto premuto dall'utente corrisponde alla lettera \"q\"(quit).\n",
    "    # La funzione ord() restituisce il valore intero che rappresenta il carattere passato come argomento, \n",
    "    # quindi ord(\"q\") restituisce il valore intero corrispondente alla lettera \"q\" (che di solito è 113 in decimale). \n",
    "    frame = resize(im_pil)\n",
    "    tensor = to_tensor(frame)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    if ret==True:\n",
    "        output, keypoints = inference(model, tensor)\n",
    "        embedding = assocEmbedding(keypoints)\n",
    "        frame = drawSkeleton(tensor[0], embedding[0])\n",
    "        #frame = drawKeypoints(tensor[0], keypoints[0])\n",
    "        cv2.imshow(\"Image Keypoints\", frame)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break         \n",
    "    # Leggo il successivo frame\n",
    "    ret, frame = webcam.read()\n",
    "\n",
    "# Chiudo la webcam\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "# Stampo le dimensioni dell'array\n",
    "print(\"Dimensioni dell'array di frame:\", len(frames))\n",
    "print(\"Dimensioni dell'array di timestamp:\", len(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adatto le immagini alle dimensioni dei tensori PyTorch \n",
    "\n",
    "display(len(frames)) # numero di frames (immagini)\n",
    "\n",
    "# Adatto i frames alle dimensioni di un tensore PyTorch \n",
    "resize = transforms.Resize([224, 224])  \n",
    "to_tensor = transforms.ToTensor() \n",
    "tensors = [] # Creo la lista di tensori\n",
    "for frame in frames:\n",
    "    frame = resize(frame) # Ridimensiono le dimensioni dei frames\n",
    "    tensor = to_tensor(frame) # Trasformo il frame in tensore\n",
    "    tensor = tensor.unsqueeze(0) # Aggiungo un'altra dimensione per far corrispondere la shape del tensore alla shape (NCHW) di un tensore PyTorch\n",
    "    tensors.append(tensor) # Aggiungo il frame convertito in tensore alla lista di tensori\n",
    "\n",
    "display(len(tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferisco i Keypoints dai frame\n",
    "\n",
    "restricted_keypoints_vec = []\n",
    "embedding_vec = []\n",
    "for tensor in tqdm(tensors):\n",
    "    output, keypoints = inference(model, tensor)\n",
    "    embedding = assocEmbedding(keypoints)\n",
    "    restricted_keypoints = [[keypoints[0][0], keypoints[0][1], keypoints[0][2], keypoints[0][3], keypoints[0][4], keypoints[0][5], keypoints[0][6],keypoints[0][7],keypoints[0][13]]]    \n",
    "    restricted_keypoints_vec.append(restricted_keypoints)\n",
    "    embedding_vec.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegno i keypoints inferiti sui frame\n",
    "\n",
    "idx = 0\n",
    "for tensor, restricted_keypoints in zip(tensors, restricted_keypoints_vec):\n",
    "    frame = drawKeypoints(tensor[idx], restricted_keypoints[idx])\n",
    "    cv2.imshow(\"Image Keypoints\", frame)\n",
    "    cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegno gli skeleton a partire dai keypoints\n",
    "\n",
    "idx = 0\n",
    "for tensor, embedding in zip(tensors, embedding_vec):\n",
    "    frame = drawSkeleton(tensor[idx], embedding[idx])\n",
    "    cv2.imshow(\"Image Keypoints\", frame)\n",
    "    cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo solo i 9 keypoints scelti per ogni frame\n",
    "\n",
    "d = {'timestamp':[],'x': [], 'y': [], 'tag':[]}\n",
    "df_keypoints = pd.DataFrame(data=d)\n",
    "for timestamp, restricted_keypoints in zip(timestamps, restricted_keypoints_vec):\n",
    "    for i in range(len(restricted_keypoints[0])):\n",
    "        if not (restricted_keypoints[0][i]):\n",
    "            df_temp = pd.DataFrame([{'timestamp': '01/01/1970 01:00:00','x':np.nan, 'y':np.nan, 'tag':np.nan}])\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "        else:\n",
    "            df_temp = pd.DataFrame(restricted_keypoints[0][i])\n",
    "            df_temp['timestamp'] = timestamp\n",
    "            df_keypoints = pd.concat([df_keypoints, df_temp])\n",
    "df_keypoints.to_csv(file_path_csv_keypoints_webcam, index=False)\n",
    "df_multiindex = df_keypoints.set_index(['timestamp'])\n",
    "display(df_multiindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115ab4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico un video da url\n",
    "url = 'https://www.youtube.com/watch?v=80z9NLFyfQU' # ~20 sec\n",
    "# url = 'https://www.youtube.com/watch?v=seTu2hiI0kE' # ~8 sec\n",
    "\n",
    "streams = streamlink.streams(url)\n",
    "\n",
    "s = streams['360p'] # higher than 360p too much time-consuming\n",
    "container = av.open(s.url, format='segment')\n",
    "\n",
    "frames = []\n",
    "for frame in container.decode(video=0):\n",
    "    frames.append(frame.to_image().convert('RGB'))\n",
    "container.close() # don't forget to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elaboro i frames\n",
    "display(len(frames)) # numero di frames (immagini)\n",
    "\n",
    "# Adatto i frames alle dimensioni di un tensore PyTorch \n",
    "resize = transforms.Resize([224, 224])  \n",
    "to_tensor = transforms.ToTensor() \n",
    "tensors = [] # Creo la lista di tensori\n",
    "for frame in frames:\n",
    "    frame = resize(frame) # Ridimensiono le dimensioni dei frames\n",
    "    tensor = to_tensor(frame) # Trasformo il frame in tensore\n",
    "    tensor = tensor.unsqueeze(0) # Aggiungo un'altra dimensione per far corrispondere la shape del tensore alla shape (NCHW) di un tensore PyTorch\n",
    "    tensors.append(tensor) # Aggiungo il frame convertito in tensore alla lista di tensori\n",
    "display(len(tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferisco i Keypoints dai frame\n",
    "\n",
    "restricted_keypoints_vec = []\n",
    "embedding_vec = []\n",
    "for tensor in tensors:\n",
    "    output, keypoints = inference(model, tensor)\n",
    "    embedding = assocEmbedding(keypoints)\n",
    "    restricted_keypoints = [[keypoints[0][0], keypoints[0][1], keypoints[0][2], keypoints[0][3], keypoints[0][4], keypoints[0][5], keypoints[0][6],keypoints[0][7],keypoints[0][13]]]    \n",
    "    restricted_keypoints_vec.append(restricted_keypoints)\n",
    "    embedding_vec.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegno i keypoints inferiti sul frame\n",
    "\n",
    "idx = 0\n",
    "for tensor, restricted_keypoints in zip(tensors, restricted_keypoints_vec):\n",
    "    frame = drawKeypoints(tensor[idx], restricted_keypoints[idx])\n",
    "    cv2.imshow(\"Image Keypoints\", frame)\n",
    "    cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da516ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
